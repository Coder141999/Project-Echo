{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimised Engine Pipeline\n",
    "\n",
    "The notebook provide an optimised version of the generic engine pipeline.\n",
    "\n",
    "This notebook aims to enhance the engine pipeline through:\n",
    "\n",
    "(1) Be compatible with Lambda Stack (https://lambdalabs.com/lambda-stack-deep-learning-software) to enable containisation, cloud and cross-platform development\n",
    "\n",
    "(2) Upgrade tensorflow from 2.10 to 2.11\n",
    "\n",
    "(3) Enable execution on linux and Windows 11 WSL2 environments\n",
    "\n",
    "(3) Minimise library dependencies\n",
    "\n",
    "(4) Solve the parallel pipeline execution issues inherent in the generic_engine_pipeline for faster execution of the pipeline\n",
    "\n",
    "Author: akudilczak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version           :  3.8.16\n",
      "TensorFlow Version       :  2.10.1\n",
      "Librosa Version          :  0.10.0.post2\n",
      "Audiomentations Version  :  0.31.0\n"
     ]
    }
   ],
   "source": [
    "########################################################################################\n",
    "# library imports\n",
    "########################################################################################\n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# environment settings\n",
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "# generic libraries\n",
    "from platform import python_version\n",
    "import functools\n",
    "from functools import lru_cache\n",
    "import diskcache as dc\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tensor flow / keras related libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_addons as tfa \n",
    "from keras.utils import dataset_utils\n",
    "\n",
    "# image processing related libraries\n",
    "import librosa\n",
    "\n",
    "# audio processing libraries\n",
    "import audiomentations\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
    "\n",
    "# print system information\n",
    "print('Python Version           : ', python_version())\n",
    "print('TensorFlow Version       : ', tf.__version__)\n",
    "print('Librosa Version          : ', librosa.__version__)\n",
    "print('Audiomentations Version  : ', audiomentations.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Configuration\n",
    "\n",
    "The following code sets up the pipeline with configuration options.\n",
    "\n",
    "The key is to set the audio data directory to the root directory containing the folders with raw audio files. \n",
    "\n",
    "This expects the folders names to be the species names.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# system constants\n",
    "########################################################################################\n",
    "\n",
    "########################################################################################\n",
    "# IMPORTANT!\n",
    "# CHANGING SOME OF THESE CONSTANT VARIABLES MAY BREAK OTHER PARTS OF THE SYSTEM.\n",
    "########################################################################################\n",
    "\n",
    "SC = {\n",
    "    'AUDIO_DATA_DIRECTORY': \"c:\\\\Users\\\\ASUS\\\\Project-Echo-Database\\\\b2\", # set the location of bucket 2 from GCP \n",
    "    'CACHE_DIRETORY': \"c:\\\\Users\\\\ASUS\\\\Project-Echo-Database\\\\pipeline_cache\", # stores image samples so it does not regenerate images each epic\n",
    "\n",
    "    'AUDIO_CLIP_DURATION': 5, # seconds \n",
    "    'AUDIO_NFFT': 2048,\n",
    "    'AUDIO_WINDOW': None,\n",
    "    'AUDIO_STRIDE': 200,\n",
    "    'AUDIO_SAMPLE_RATE': 48000,\n",
    "    'AUDIO_MELS': 260,\n",
    "    'AUDIO_FMIN': 20,\n",
    "    'AUDIO_FMAX': 13000,\n",
    "    'AUDIO_TOP_DB': 80,\n",
    "\n",
    "    'MODEL_INPUT_IMAGE_WIDTH': 260,\n",
    "    'MODEL_INPUT_IMAGE_HEIGHT': 260,\n",
    "    'MODEL_INPUT_IMAGE_CHANNELS': 3,\n",
    "\n",
    "    'USE_DISK_CACHE': True, #switch off disc cache here if preferred\n",
    "    'SAMPLE_VARIANTS': 20,\n",
    "    'CLASSIFIER_BATCH_SIZE': 16,\n",
    "    'MAX_EPOCHS': 5000,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limit Memory\n",
    "\n",
    "The following limits memory. Helpful if you have a small PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_memory_limit(mem_mb):\n",
    "  # enforce memory limit on GPU\n",
    "\n",
    "  gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "  if gpus:\n",
    "    try:\n",
    "      tf.config.experimental.set_virtual_device_configuration(\n",
    "          gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=mem_mb)])\n",
    "      print(f\"vram limit set to {mem_mb}MB\")\n",
    "    except RuntimeError as e:\n",
    "      print(e)\n",
    "      \n",
    "# enforce max 5GB memory on GPU for this notebook if you have a small GPU\n",
    "# enforce_memory_limit(5120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disk Caching\n",
    "\n",
    "The following code creates a disk cache.  You will need lots of space (20 GB+) if you create large melspectrograms.\n",
    "\n",
    "The caching works by serialising a function call signature and hashing it into a key.  This key is used to store the result of the function call.\n",
    "\n",
    "This allows the a result from the cache to be utilised instead of calling the function, which means the entire data processing pipeline can be cached if used correctly.\n",
    "\n",
    "This works best when the function being cached is idempotent.  There may be circumstances where it doesn't matter.  Be careful with using this cache as you may get unexpected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Create a DiskCache instance\n",
    "# This cache will allow us store intermediate function results to speed up the \n",
    "# data processing pipeline\n",
    "########################################################################################\n",
    "if SC['USE_DISK_CACHE']:\n",
    "    cache = dc.Cache(SC['CACHE_DIRETORY'], cull_limit=0, size_limit=10**9) \n",
    "\n",
    "\n",
    "########################################################################################\n",
    "# a helper function to create a hash key from a function signature and arguments\n",
    "########################################################################################\n",
    "def create_function_key(func, *args, **kwargs):\n",
    "    partial_func = functools.partial(func, *args, **kwargs)\n",
    "    func_name = partial_func.func.__name__\n",
    "    func_module = partial_func.func.__module__\n",
    "    args_repr = repr(partial_func.args)\n",
    "    kwargs_repr = repr(sorted(partial_func.keywords.items()))\n",
    "\n",
    "    key = f\"{func_module}.{func_name}:{args_repr}:{kwargs_repr}\"\n",
    "    # Use hashlib to create a hash of the key for shorter and consistent length\n",
    "    key_hash = hashlib.sha256(key.encode()).hexdigest()\n",
    "\n",
    "    return key, key_hash, partial_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the files into a Tensorflow dataset structure for model training\n",
    "\n",
    "This initial code loads only the filenames.  The filenames are then split into train, validation and test datasets.  This is designed deliberately this way to conserve runtime memory.\n",
    "\n",
    "Subsequent downstream loading of the file content occurs as part of the data pipeline transformation 'map' function.  See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# these helper functions load the audio data into a 'dataset' using only paths\n",
    "# just dealing with paths at this early stage means the entire dataset can be shuffled in\n",
    "# memory and split before loading the actual audio data into memory\n",
    "########################################################################################\n",
    "def paths_and_labels_to_dataset(image_paths, labels, num_classes):\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "    label_ds = dataset_utils.labels_to_dataset(\n",
    "        labels, \n",
    "        'categorical', \n",
    "        num_classes)\n",
    "    zipped_path_ds = tf.data.Dataset.zip((path_ds, label_ds))\n",
    "    return zipped_path_ds\n",
    "\n",
    "# The below code loads the data from the folder above\n",
    "# Creates a list of all the class names\n",
    "# Creates dataset for training, validation and test\n",
    "def create_datasets(audio_files, train_split=0.7, val_split=0.2):\n",
    "    file_paths, labels, class_names = dataset_utils.index_directory(\n",
    "            audio_files,\n",
    "            labels=\"inferred\",\n",
    "            formats=('.ogg','.mp3','.wav','.flac'),\n",
    "            class_names=None,\n",
    "            shuffle=True,\n",
    "            seed=42,\n",
    "            follow_links=False)\n",
    "\n",
    "    dataset = paths_and_labels_to_dataset(\n",
    "        image_paths=file_paths,\n",
    "        labels=labels,\n",
    "        num_classes=len(class_names))\n",
    "    \n",
    "    # Calculate the size of the dataset\n",
    "    dataset_size = len(dataset)\n",
    "    \n",
    "    # Calculate the number of elements for each dataset split\n",
    "    train_size = int(train_split * dataset_size)\n",
    "    val_size = int(val_split * dataset_size)\n",
    "    test_size = dataset_size - train_size - val_size\n",
    "\n",
    "    # Split the dataset\n",
    "    train_ds = dataset.take(train_size)\n",
    "    val_ds = dataset.skip(train_size).take(val_size)\n",
    "    test_ds = dataset.skip(train_size + val_size).take(test_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 962 files belonging to 16 classes.\n",
      "Class names:  ['Aegotheles cristatus Australian owlet-nightjar', 'Alauda arvensis European Skylark', 'Caligavis chrysops Yellow-faced honeyeater', 'Capra hircus Feral goat', 'Cervus unicolour Sambar deer', 'Colluricincla harmonica Grey shrikethrush', 'Corvus coronoides Australian raven', 'Dama dama Fallow Deer', 'Eopsaltria australis Eastern yellow robin', 'Felis Catus Cat', 'Pachycephala rufiventris Rufous whistler', 'Ptilotula penicillata White-plumed honeyeater', 'Rattus norvegicus Brown rat', 'Strepera graculina Pied currawong', 'sus scrofa Wild pig', 'vulpes vulpes red fox']\n",
      "Training   dataset length: 769\n",
      "Validation dataset length: 182\n",
      "Test       dataset length: 11\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "train_ds, val_ds, test_ds, class_names = create_datasets(SC['AUDIO_DATA_DIRECTORY'],train_split=0.8, val_split=0.19)\n",
    "print(\"Class names: \", class_names)\n",
    "print(f\"Training   dataset length: {len(train_ds)}\")\n",
    "print(f\"Validation dataset length: {len(val_ds)}\")\n",
    "print(f\"Test       dataset length: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'c:\\\\Users\\\\ASUS\\\\Project-Echo-Database\\\\b2\\\\Corvus coronoides Australian raven\\\\X03898.mp3'>, <tf.Tensor: shape=(16,), dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'c:\\\\Users\\\\ASUS\\\\Project-Echo-Database\\\\b2\\\\Dama dama Fallow Deer\\\\jelenie-deer-69860.wav'>, <tf.Tensor: shape=(16,), dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'c:\\\\Users\\\\ASUS\\\\Project-Echo-Database\\\\b2\\\\Dama dama Fallow Deer\\\\1770_Damhirsch_Jungtier_Rufe.mp3'>, <tf.Tensor: shape=(16,), dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'c:\\\\Users\\\\ASUS\\\\Project-Echo-Database\\\\b2\\\\Ptilotula penicillata White-plumed honeyeater\\\\X03557.mp3'>, <tf.Tensor: shape=(16,), dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "      dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'c:\\\\Users\\\\ASUS\\\\Project-Echo-Database\\\\b2\\\\Colluricincla harmonica Grey shrikethrush\\\\X03685.mp3'>, <tf.Tensor: shape=(16,), dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'c:\\\\Users\\\\ASUS\\\\Project-Echo-Database\\\\b2\\\\Pachycephala rufiventris Rufous whistler\\\\X01212.mp3'>, <tf.Tensor: shape=(16,), dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'c:\\\\Users\\\\ASUS\\\\Project-Echo-Database\\\\b2\\\\Cervus unicolour Sambar deer\\\\7.wav'>, <tf.Tensor: shape=(16,), dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'c:\\\\Users\\\\ASUS\\\\Project-Echo-Database\\\\b2\\\\Felis Catus Cat\\\\cat_111.wav'>, <tf.Tensor: shape=(16,), dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'c:\\\\Users\\\\ASUS\\\\Project-Echo-Database\\\\b2\\\\Alauda arvensis European Skylark\\\\Alauda_arvensis_PF00140_short.mp3'>, <tf.Tensor: shape=(16,), dtype=float32, numpy=\n",
      "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'c:\\\\Users\\\\ASUS\\\\Project-Echo-Database\\\\b2\\\\Colluricincla harmonica Grey shrikethrush\\\\X03694.mp3'>, <tf.Tensor: shape=(16,), dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "# Currently just a list of paths\n",
    "# This code shows what the pipeline looks like at this stage\n",
    "# Shows the tensor with the path to the audio file\n",
    "# Another tensor with dimension 15 which means it's one-hot encoded\n",
    "# The no. 1 in the the array tells you what class it is\n",
    "\n",
    "for item in train_ds.take(10): #take 10 from training set\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next few functions take the same signature and always return the same parameters.\n",
    "# Essentially loads the audio files, if the final result is cached then it skips the remainder of the function.\n",
    "# If not in cache then it processes it.\n",
    "\n",
    "# enable this cache if using large audio files, and lots of available RAM\n",
    "def python_load_and_decode_file(sample, label, variant, cache_key, cache_found):\n",
    "    \n",
    "    if cache_found == np.int32(0): # Simply load the final result if in cache\n",
    "        \n",
    "        tmp_audio_t = None\n",
    "        \n",
    "        with open(sample, 'rb') as file:\n",
    "\n",
    "            # Load the audio data with librosa\n",
    "            tmp_audio_t, _ = librosa.load(file, sr=SC['AUDIO_SAMPLE_RATE'])\n",
    "            \n",
    "            # cast and keep right channel only\n",
    "            if tmp_audio_t.ndim == 2 and tmp_audio_t.shape[0] == 2:\n",
    "                tmp_audio_t = tmp_audio_t[1, :]\n",
    "            \n",
    "            # cast and keep right channel only\n",
    "            tmp_audio_t = tmp_audio_t.astype(np.float32)\n",
    "                    \n",
    "            assert(tmp_audio_t is not None)\n",
    "            assert(isinstance(tmp_audio_t, np.ndarray))\n",
    "        \n",
    "        sample = tmp_audio_t\n",
    "        \n",
    "    else:\n",
    "        sample = cache[cache_key.decode('utf-8')]\n",
    "    \n",
    "    return sample, label, variant, cache_key, cache_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling the audio files\n",
    "\n",
    "The function loads and decodes an audio file from the given path, calculates the audio file's duration in seconds, and then extracts a random subsection of the specified duration (in seconds) from the audio. If the audio duration is shorter than the specified duration, the function pads the subsection with silence to meet the required length. The resulting subsection is returned as a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorflow_load_random_subsection(sample, label, variant, cache_key, cache_found):\n",
    "    \n",
    "    if cache_found == np.int32(0):\n",
    "        duration_secs = SC['AUDIO_CLIP_DURATION']\n",
    "        \n",
    "        # Determine the audio file's duration in seconds\n",
    "        audio_duration_secs = tf.shape(sample)[0] / SC['AUDIO_SAMPLE_RATE']\n",
    "        \n",
    "        if audio_duration_secs>duration_secs:\n",
    "        \n",
    "            # Calculate the starting point of the 5-second subsection\n",
    "            max_start = tf.cast(audio_duration_secs - duration_secs, tf.float32)\n",
    "            start_time_secs = tf.random.uniform((), 0.0, max_start, dtype=tf.float32)\n",
    "            \n",
    "            start_index = tf.cast(start_time_secs * SC['AUDIO_SAMPLE_RATE'], dtype=tf.int32)\n",
    "    \n",
    "            # Load the 5-second subsection\n",
    "            end_index = tf.cast(start_index + tf.cast(duration_secs, tf.int32) * SC['AUDIO_SAMPLE_RATE'], tf.int32)\n",
    "            \n",
    "            subsection = sample[start_index : end_index]\n",
    "        \n",
    "        else:\n",
    "            # Pad the subsection with silence if it's shorter than 5 seconds\n",
    "            padding_length = duration_secs * SC['AUDIO_SAMPLE_RATE'] - tf.shape(sample)[0]\n",
    "            padding = tf.zeros([padding_length], dtype=sample.dtype)\n",
    "            subsection = tf.concat([sample, padding], axis=0)\n",
    "\n",
    "        sample = subsection\n",
    "\n",
    "    return sample, label, variant, cache_key, cache_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio augmentations\n",
    "\n",
    "The following code applies a sequence of augmentations to the audio signal.\n",
    "\n",
    "A probability of applying the augmentation is used to ensure the augmentation isn't applied every sample.\n",
    "\n",
    "This means there will be some samples that go straight through with no augmentations and a small probability that in fact all augmentations will be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio augmentation pipeline\n",
    "# You can specifiy at what probability should it change i.e. p=0.2\n",
    "\n",
    "def python_audio_augmentations(sample, label, variant, cache_key, cache_found):\n",
    "    \n",
    "    if cache_found == np.int32(0):\n",
    "        # See https://github.com/iver56/audiomentations for more options\n",
    "        augmentations = Compose([\n",
    "            # Add Gaussian noise with a random amplitude to the audio\n",
    "            # This can help the model generalize to real-world scenarios where noise is present\n",
    "            AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.2),\n",
    "\n",
    "            # Time-stretch the audio without changing its pitch\n",
    "            # This can help the model become invariant to small changes in the speed of the audio\n",
    "            TimeStretch(min_rate=0.8, max_rate=1.25, p=0.2),\n",
    "\n",
    "            # Shift the pitch of the audio within a range of semitones\n",
    "            # This can help the model generalize to variations in pitch that may occur in real-world scenarios\n",
    "            PitchShift(min_semitones=-4, max_semitones=4, p=0.2),\n",
    "\n",
    "            # Shift the audio in time by a random fraction\n",
    "            # This can help the model become invariant to the position of important features in the audio\n",
    "            Shift(min_fraction=-0.5, max_fraction=0.5, p=0.2),\n",
    "        ])\n",
    "        \n",
    "        # apply audio augmentation to the clip\n",
    "        # note: this augmentation is NOT applied in the test and validation pipelines\n",
    "        sample = augmentations(samples=sample, sample_rate=SC['AUDIO_SAMPLE_RATE'])\n",
    "    \n",
    "    return sample, label, variant, cache_key, cache_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual dataset pipelines\n",
    "\n",
    "When it first loads off the disk, it only has the path and the label. This function takes the path and the label and adds variant, cache_key, cache_found. This creates the signature that is needed for the other functions. \n",
    "\n",
    "The file is the dataset is split into different pipeline function as that augmentations would be applied to the training data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorflow_add_variant_and_cache(path, label):\n",
    "    variant     = tf.random.uniform(shape=(), minval=0, maxval=SC['SAMPLE_VARIANTS'], dtype=tf.int32)\n",
    "    sample      = path\n",
    "    cache_key   = b'no key'\n",
    "    cache_found = np.int32(0)\n",
    "    return sample, label, variant, cache_key, cache_found\n",
    "\n",
    "# Helper functions\n",
    "def tensorflow_drop_variant_and_cache(sample, label, variant, cache_key, cache_found):\n",
    "    return sample, label\n",
    "\n",
    "def tensorflow_output_shape_setter(sample, label, variant, cache_key, cache_found):\n",
    "    sample.set_shape([SC['MODEL_INPUT_IMAGE_WIDTH'], SC['MODEL_INPUT_IMAGE_HEIGHT'], SC['MODEL_INPUT_IMAGE_CHANNELS']])\n",
    "    label.set_shape([len(class_names),]) \n",
    "    return sample, label, variant, cache_key, cache_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_function_wrapper(pipeline_fn, out_types, sample, label, variant, cache_key, cache_found):\n",
    "\n",
    "    # Use a lambda function to pass two arguments to the function\n",
    "    sample, label, variant, cache_key, cache_found = tf.numpy_function(\n",
    "        func=lambda v1,v2,v3,v4,v5: pipeline_fn(v1,v2,v3,v4,v5),\n",
    "        inp=(sample, label, variant, cache_key, cache_found),\n",
    "        Tout=out_types)\n",
    "\n",
    "    return sample, label, variant, cache_key, cache_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_disk_cache_start(sample, label, variant, cache_key, cache_found):\n",
    "\n",
    "    cache_key   = b'no key'\n",
    "    cache_found = np.int32(0)\n",
    "    \n",
    "    if SC['USE_DISK_CACHE']:\n",
    "        _,cache_key,_ = create_function_key(python_disk_cache_start, sample, label, variant)\n",
    "        if cache_key in cache:\n",
    "            #print(f'found {cache_key} in cache')\n",
    "            cache_found = np.int32(1)\n",
    "        else:\n",
    "            pass\n",
    "            #print(f'{cache_key} not found in cache')\n",
    "            \n",
    "    return sample, label, variant, cache_key, cache_found\n",
    "\n",
    "def python_disk_cache_end(sample, label, variant, cache_key, cache_found):\n",
    "    cache_key = cache_key.decode('utf-8')\n",
    "    if SC['USE_DISK_CACHE']:\n",
    "        # if it was not found in the cache at the start, then populate with what we built\n",
    "        # during the pipeline execution\n",
    "        if cache_found == np.int32(0):\n",
    "            #print(f'adding {cache_key} to cache')\n",
    "            cache[cache_key] = sample\n",
    "        #else:\n",
    "        #    sample = cache[cache_key]\n",
    "            \n",
    "    return sample, label, variant, cache_key, cache_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Create the datasets necessary for training a classification model\n",
    "# Note: python and tensorflow functions are treated differently in the tensorflow\n",
    "# pipeline.  Each python function needs to be wrapped. \n",
    "# this is why each pipeline function starts with python_ or tensorflow_ to make it clear \n",
    "########################################################################################\n",
    "\n",
    "# Get the length of the training dataset\n",
    "len_train_ds = len(train_ds)\n",
    "parallel_calls = tf.data.AUTOTUNE\n",
    "cache_output_types = (tf.string,tf.float32,tf.int32,tf.string,tf.int32)\n",
    "procs_output_types = (tf.float32,tf.float32,tf.int32,tf.string,tf.int32)\n",
    "\n",
    "# Create the training dataset pipeline\n",
    "train_dataset = (train_ds\n",
    "                 # Shuffles all of the file names\n",
    "                 .shuffle(len_train_ds)\n",
    "                 # Adds variant and cache, loads file etc\n",
    "                 .map(tensorflow_add_variant_and_cache, num_parallel_calls=parallel_calls)\n",
    "                 .map(functools.partial(python_function_wrapper, python_disk_cache_start, cache_output_types), num_parallel_calls=parallel_calls)\n",
    "                 .map(functools.partial(python_function_wrapper, python_load_and_decode_file, procs_output_types), num_parallel_calls=parallel_calls)\n",
    "                 # Completes random subsection\n",
    "                 .map(tensorflow_load_random_subsection, num_parallel_calls=parallel_calls)\n",
    "                 # Completes audio augmentations\n",
    "                 .map(functools.partial(python_function_wrapper, python_audio_augmentations, procs_output_types), num_parallel_calls=parallel_calls)\n",
    "                 \n",
    "                \n",
    "                 # Store in cache if new\n",
    "                 .map(functools.partial(python_function_wrapper, python_disk_cache_end, procs_output_types), num_parallel_calls=parallel_calls)\n",
    "                 # What is the output shape\n",
    "                 .map(tensorflow_output_shape_setter, num_parallel_calls=parallel_calls)\n",
    "                 # Drop variant and cache information as it's not required anymore\n",
    "                 .map(tensorflow_drop_variant_and_cache, num_parallel_calls=parallel_calls)\n",
    "                 # Ready to pass into the model\n",
    "                 .batch(SC['CLASSIFIER_BATCH_SIZE'])\n",
    "                 # Start preparing the next set of data\n",
    "                 .prefetch(parallel_calls)\n",
    "                 .repeat(count=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the validation dataset pipeline\n",
    "validation_dataset = (val_ds\n",
    "                    .map(tensorflow_add_variant_and_cache, num_parallel_calls=parallel_calls)\n",
    "                    .map(functools.partial(python_function_wrapper, python_disk_cache_start, cache_output_types), num_parallel_calls=parallel_calls)\n",
    "                    .map(functools.partial(python_function_wrapper, python_load_and_decode_file, procs_output_types), num_parallel_calls=parallel_calls)\n",
    "                    .map(tensorflow_load_random_subsection, num_parallel_calls=parallel_calls)\n",
    "                    \n",
    "                    .map(functools.partial(python_function_wrapper, python_disk_cache_end, procs_output_types), num_parallel_calls=parallel_calls)\n",
    "                    .map(tensorflow_output_shape_setter, num_parallel_calls=parallel_calls)\n",
    "                    .map(tensorflow_drop_variant_and_cache, num_parallel_calls=parallel_calls)\n",
    "                    .batch(SC['CLASSIFIER_BATCH_SIZE'])\n",
    "                    .prefetch(parallel_calls)\n",
    "                    .repeat(count=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the test dataset pipeline\n",
    "test_dataset = (test_ds\n",
    "                 .map(tensorflow_add_variant_and_cache, num_parallel_calls=parallel_calls)\n",
    "                 .map(functools.partial(python_function_wrapper, python_disk_cache_start, cache_output_types), num_parallel_calls=parallel_calls)\n",
    "                 .map(functools.partial(python_function_wrapper, python_load_and_decode_file, procs_output_types), num_parallel_calls=parallel_calls)\n",
    "                 .map(tensorflow_load_random_subsection, num_parallel_calls=parallel_calls)\n",
    "                 \n",
    "                 .map(functools.partial(python_function_wrapper, python_disk_cache_end, procs_output_types), num_parallel_calls=parallel_calls)\n",
    "                 .map(tensorflow_output_shape_setter, num_parallel_calls=parallel_calls)\n",
    "                 .map(tensorflow_drop_variant_and_cache, num_parallel_calls=parallel_calls)\n",
    "                 .batch(SC['CLASSIFIER_BATCH_SIZE'])\n",
    "                 .prefetch(parallel_calls)\n",
    "                 .repeat(count=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model\n",
    "\n",
    "Now that the images are produced, it moves into an image classification model.\n",
    "\n",
    "In order to test whether the pipeline is working a CNN based image classification model is constructed below.  This model leverages pre-trained model weights for the EfficientNetV2 feature model which generates a vector representation of 1000 floats for each input image.  Note that on first run this model tensorflow hub library will check if model weights are available, and if not, will automatically download them to your computer.  This may take a few minutes the first time this is run.\n",
    "\n",
    "The output from the EfficientNetV2 model is then fed into 2 fully connected layers to perform the classification function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Assuming SC and class_names are defined elsewhere in the notebook\n",
    "# For the purpose of demonstration, I'll define dummy values here.\n",
    "SC = {\n",
    "    'MODEL_INPUT_IMAGE_HEIGHT': 96,\n",
    "    'MODEL_INPUT_IMAGE_WIDTH': 64,\n",
    "    'MODEL_INPUT_IMAGE_CHANNELS': 1\n",
    "}\n",
    "class_names = [\"class1\", \"class2\", \"class3\"]  # Example classes\n",
    "\n",
    "def build_vggish_model(trainable):\n",
    "    # Build a classification model using a pre-trained VGGish\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # Input layer with specified audio dimensions\n",
    "            tf.keras.layers.InputLayer(input_shape=(SC['MODEL_INPUT_IMAGE_HEIGHT'], \n",
    "                                                    SC['MODEL_INPUT_IMAGE_WIDTH'], \n",
    "                                                    SC['MODEL_INPUT_IMAGE_CHANNELS'])),\n",
    "            \n",
    "            # Use the VGGish model as a feature generator\n",
    "            hub.KerasLayer(\"https://tfhub.dev/google/vggish/1\", trainable=trainable),\n",
    "\n",
    "            # Add the classification layers\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "            # Fully connected layer with multiple of the number of classes\n",
    "            tf.keras.layers.Dense(len(class_names) * 8,\n",
    "                                  activation=\"relu\"),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "            # Another fully connected layer with multiple of the number of classes\n",
    "            tf.keras.layers.Dense(len(class_names) * 4,\n",
    "                                  activation=\"relu\"),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "            # Add dropout to reduce overfitting\n",
    "            tf.keras.layers.Dropout(0.50),\n",
    "\n",
    "            # Output layer with one node per class, using softmax activation for multiclass classification\n",
    "            tf.keras.layers.Dense(len(class_names), activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Set the input shape for the model\n",
    "    model.build([None, \n",
    "                 SC['MODEL_INPUT_IMAGE_HEIGHT'],\n",
    "                 SC['MODEL_INPUT_IMAGE_WIDTH'], \n",
    "                 SC['MODEL_INPUT_IMAGE_CHANNELS']])\n",
    "\n",
    "    # Display the model summary\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Uncomment below to test the function\n",
    "# build_vggish_model(trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "The following sets up callbacks to track model training and performs the model fit to train the model.\n",
    "\n",
    "The callbacks will ensure the best model weights (as defined by lowest validation loss) will be written to disk during training.  This is important as the model training could take several hours (12+ hours) to complete and any errors may cause the training loop to exit - so having the checkpoints acts as a backup.  The model will stop training when it sees no further improvement to the validation loss, after which it will restore the best weights found.  This allows the training to discover when it is overfit and stop further training.  This is why the number of epics is 10000.  It is expected the model training will end significantly earlier than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:absl:hub.KerasLayer is trainable but has zero trainable weights.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"keras_layer\" (type KerasLayer).\n\nin user code:\n\n    File \"C:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\", line 234, in call  *\n        result = f()\n\n    ValueError: Python inputs incompatible with input_signature:\n      inputs: (\n        Tensor(\"Placeholder:0\", shape=(None, 96, 64, 1), dtype=float32))\n      input_signature: (\n        TensorSpec(shape=(None,), dtype=tf.float32, name=None)).\n\n\nCall arguments received by layer \"keras_layer\" (type KerasLayer):\n  • inputs=tf.Tensor(shape=(None, 96, 64, 1), dtype=float32)\n  • training=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/1\u001b[39m\u001b[38;5;124m'\u001b[39m)    \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# allow all the weights to be trained\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_vggish_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# the form_logits means the loss function has the 'softmax' buillt in.  This approach is numerically more stable\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# than including the softmax activation on the last layer of the classifier\u001b[39;00m\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), \n\u001b[0;32m     12\u001b[0m               optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m), \n\u001b[0;32m     13\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     14\u001b[0m               )\n",
      "Cell \u001b[1;32mIn[17], line 15\u001b[0m, in \u001b[0;36mbuild_vggish_model\u001b[1;34m(trainable)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_vggish_model\u001b[39m(trainable):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Build a classification model using a pre-trained VGGish\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Input layer with specified audio dimensions\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInputLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSC\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMODEL_INPUT_IMAGE_HEIGHT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mSC\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMODEL_INPUT_IMAGE_WIDTH\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mSC\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMODEL_INPUT_IMAGE_CHANNELS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m            \u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Use the VGGish model as a feature generator\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKerasLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://tfhub.dev/google/vggish/1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Add the classification layers\u001b[39;49;00m\n\u001b[0;32m     26\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFlatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBatchNormalization\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Fully connected layer with multiple of the number of classes\u001b[39;49;00m\n\u001b[0;32m     30\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBatchNormalization\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \n\u001b[0;32m     34\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Another fully connected layer with multiple of the number of classes\u001b[39;49;00m\n\u001b[0;32m     35\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBatchNormalization\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Add dropout to reduce overfitting\u001b[39;49;00m\n\u001b[0;32m     40\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.50\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Output layer with one node per class, using softmax activation for multiclass classification\u001b[39;49;00m\n\u001b[0;32m     43\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msoftmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# Set the input shape for the model\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     model\u001b[38;5;241m.\u001b[39mbuild([\u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[0;32m     49\u001b[0m                  SC[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMODEL_INPUT_IMAGE_HEIGHT\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     50\u001b[0m                  SC[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMODEL_INPUT_IMAGE_WIDTH\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m     51\u001b[0m                  SC[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMODEL_INPUT_IMAGE_CHANNELS\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file8n02_q68.py:74\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m     72\u001b[0m     result \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(smart_cond)\u001b[38;5;241m.\u001b[39msmart_cond, (ag__\u001b[38;5;241m.\u001b[39mld(training), ag__\u001b[38;5;241m.\u001b[39mautograph_artifact((\u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(f), (), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope))), ag__\u001b[38;5;241m.\u001b[39mautograph_artifact((\u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(f), (), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), fscope)))), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     73\u001b[0m result \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 74\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mnot_(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_has_training_argument), if_body_3, else_body_3, get_state_3, set_state_3, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_6\u001b[39m():\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (result,)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file8n02_q68.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.if_body_3\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mif_body_3\u001b[39m():\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m training, result\n\u001b[1;32m---> 37\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"keras_layer\" (type KerasLayer).\n\nin user code:\n\n    File \"C:\\ProgramData\\Anaconda3\\envs\\dev\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\", line 234, in call  *\n        result = f()\n\n    ValueError: Python inputs incompatible with input_signature:\n      inputs: (\n        Tensor(\"Placeholder:0\", shape=(None, 96, 64, 1), dtype=float32))\n      input_signature: (\n        TensorSpec(shape=(None,), dtype=tf.float32, name=None)).\n\n\nCall arguments received by layer \"keras_layer\" (type KerasLayer):\n  • inputs=tf.Tensor(shape=(None, 96, 64, 1), dtype=float32)\n  • training=None"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('models/'):\n",
    "    os.mkdir('models/')\n",
    "if not os.path.exists('models/1'):\n",
    "    os.mkdir('models/1')    \n",
    "    \n",
    "# allow all the weights to be trained\n",
    "model = build_vggish_model(True)\n",
    "\n",
    "# the form_logits means the loss function has the 'softmax' buillt in.  This approach is numerically more stable\n",
    "# than including the softmax activation on the last layer of the classifier\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), \n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), \n",
    "              metrics=[\"accuracy\"],\n",
    "              )\n",
    "\n",
    "# tensorboard for visualisation of results\n",
    "log_dir = \"tensorboard_logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, \n",
    "                                                      histogram_freq=1)\n",
    "\n",
    "# reduce learning rate to avoid overshooting local minima\n",
    "lr_reduce_plateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                                      factor=0.75,\n",
    "                                                      patience=8, \n",
    "                                                      verbose=1,\n",
    "                                                      mode='min',\n",
    "                                                      cooldown=0, \n",
    "                                                      min_lr=1e-7)\n",
    "\n",
    "# end the training if no improvement for 16 epochs in a row, then restore best model weights\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=16,\n",
    "    verbose=0,\n",
    "    mode=\"min\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "# save the best model as it trains..\n",
    "mcp_save = tf.keras.callbacks.ModelCheckpoint('models/checkpoint_generic_model.hdf5', \n",
    "                                           save_best_only=True, \n",
    "                                           monitor='val_loss', \n",
    "                                           mode='min')\n",
    "\n",
    "# any changes to the source code will generally require the disk cache to be cleared.\n",
    "# So to be safe, the cache is cleared before training the model.  If you are sure\n",
    "# the cache is still valid then comment out this code\n",
    "# the first few epochs of the model training will be slow as the cache is populated with pipeline samples\n",
    "# and will depend on the dataset size and the number of variants included\n",
    "cache.clear()\n",
    "\n",
    "# fit the model to the training set\n",
    "# this may take 12-24 hours to run to full model convergence depending on your machine\n",
    "model.fit(train_dataset, \n",
    "          validation_data=validation_dataset,\n",
    "          callbacks=[lr_reduce_plateau, early_stopping, tensorboard_callback, mcp_save],\n",
    "          epochs=SC['MAX_EPOCHS'])      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Predictions on test data\n",
    "\n",
    "The following provides an example of how to use the trained model to perform inference (predictions).  It calculates the best class and the probability for that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict class and probability given a prediction\n",
    "def predict_class(predictions):\n",
    "    # Get the index of the class with the highest predicted probability\n",
    "    predicted_index = int(tf.argmax(tf.squeeze(predictions)).numpy())\n",
    "    # Get the class name using the predicted index\n",
    "    predicted_class = class_names[predicted_index]\n",
    "    # Calculate the predicted probability for the selected class\n",
    "    predicted_probability = 100.0 * tf.nn.softmax(predictions)[predicted_index].numpy()\n",
    "    # Round the probability to 2 decimal places\n",
    "    predicted_probability = str(round(predicted_probability, 2))\n",
    "    return predicted_class, predicted_probability\n",
    "\n",
    "# Display class names and run prediction on test entries\n",
    "print(f'Class names: {class_names}')\n",
    "for features, labels in test_dataset:\n",
    "    # Generate predictions for the given features\n",
    "    predictions = model.predict(features, verbose=0)\n",
    "\n",
    "    # Iterate over each item in the batch\n",
    "    for batch_idx in range(predictions.shape[0]):\n",
    "        # Get the index of the true class\n",
    "        true_index = int(tf.argmax(tf.squeeze(labels[batch_idx])).numpy())\n",
    "        # Get the true class name using the true index\n",
    "        true_class = class_names[true_index]\n",
    "\n",
    "        # Predict class and probability using the prediction function\n",
    "        predicted_class, predicted_probability = predict_class(predictions[batch_idx])\n",
    "\n",
    "        print(f'True class      : {true_class}')\n",
    "        print(f'Predicted class : {predicted_class}')\n",
    "        print(f'Predicted probability : {predicted_probability}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the full model for use with tensorflow serving\n",
    "model.save('models/echo_model/1/', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Students Note:\n",
    "This is step 1 of engine complete - how to create and save a model. Now you need to head over to echo_engine.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
