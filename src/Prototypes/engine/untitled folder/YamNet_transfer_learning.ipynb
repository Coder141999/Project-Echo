{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement a solution that works for your exact case, we would need more detailed information about your exact setup. However, I'll provide a more comprehensive code snippet below that you can adapt to your case. This assumes you have a directory structure where each subdirectory's name is the class label, and each subdirectory contains the corresponding audio files. The structure would look something like this:\n",
    "\n",
    "\n",
    "- main_directory\n",
    "    - class1\n",
    "        - file1.wav\n",
    "        - file2.wav\n",
    "        ...\n",
    "    - class2\n",
    "        - file1.wav\n",
    "        - file2.wav\n",
    "        ...\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook and video, we are going to do three things:\n",
    "\n",
    "1. Load and use the YAMNet model for inference.\n",
    "\n",
    "2. Build a new model using the YAMNet embeddings to classify cat and dog sounds.\n",
    "\n",
    "3. Evaluate and export your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YAMNet\n",
    "\n",
    "YAMNet is a pre-trained deep neural network that can predict audio events from 521 classes, such as laughter, barking, or a siren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In detail\n",
    "\n",
    "YAMNet is a pre-trained neural network that employs the MobileNetV1 depthwise-separable convolution architecture. It can use an audio waveform as input and make independent predictions for each of the 521 audio events from the AudioSet corpus.\n",
    "\n",
    "Internally, the model extracts \"frames\" from the audio signal and processes batches of these frames. This version of the model uses frames that are 0.96 second long and extracts one frame every 0.48 seconds .\n",
    "\n",
    "The model accepts a 1-D float32 Tensor or NumPy array containing a waveform of arbitrary length, represented as single-channel (mono) 16 kHz samples in the range [-1.0, +1.0]. This tutorial contains code to help you convert WAV files into the supported format.\n",
    "\n",
    "The model returns 3 outputs, including the class scores, embeddings (which you will use for transfer learning), and the log mel spectrogram. You can find more details here.\n",
    "\n",
    "One specific use of YAMNet is as a high-level feature extractor - the 1,024-dimensional embedding output. You will use the base (YAMNet) model's input features and feed them into your shallower model consisting of one hidden tf.keras.layers.Dense layer. Then, you will train the network on a small amount of data for audio classification without requiring a lot of labeled data and training end-to-end. (This is similar to transfer learning for image classification with TensorFlow Hub for more information.)\n",
    "\n",
    "First, we will test the model and see the results of classifying audio. You will then construct the data pre-processing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading YAMNet from TensorFlow Hub\n",
    "\n",
    "You are going to use a pre-trained YAMNet from Tensorflow Hub to extract the embeddings from the sound files.\n",
    "\n",
    "Loading a model from TensorFlow Hub is straightforward: choose the model, copy its URL, and use the load function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_io as tfio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-02 14:25:26.537850: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-08-02 14:25:26.565143: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-08-02 14:25:26.574543: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from scipy.signal import resample_poly\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "# Load YAMNet model\n",
    "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet_model = hub.load(yamnet_model_handle)\n",
    "\n",
    "# Constants\n",
    "main_directory = '/Users/ankush/Downloads/deakin-units/data/b3'\n",
    "class_names = sorted(os.listdir(main_directory))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to get file paths and labels\n",
    "def get_file_paths_and_labels(main_directory, class_names):\n",
    "    filenames = []\n",
    "    labels = []\n",
    "    for label, class_name in enumerate(class_names):\n",
    "        class_directory = os.path.join(main_directory, class_name)\n",
    "        if os.path.isdir(class_directory): # Ensure it's a directory\n",
    "            for file_name in os.listdir(class_directory):\n",
    "                if file_name.endswith('.wav'):\n",
    "                    filenames.append(os.path.join(class_directory, file_name))\n",
    "                    labels.append(label)\n",
    "    return filenames, labels\n",
    "from scipy.signal import resample\n",
    "\n",
    "def resample_audio(wav, num_samples):\n",
    "    return resample(wav, num_samples)\n",
    "\n",
    "def load_wav_16k_mono(filename, target_length=16000):\n",
    "    \"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio, and pad to target length. \"\"\"\n",
    "    file_contents = tf.io.read_file(filename)\n",
    "    wav, sample_rate = tf.audio.decode_wav(\n",
    "          file_contents,\n",
    "          desired_channels=1)\n",
    "    wav = tf.squeeze(wav, axis=-1)\n",
    "    sample_rate = tf.cast(sample_rate, dtype=tf.float32)\n",
    "    target_sample_rate = 16000.0\n",
    "\n",
    "    # Compute the number of samples for the target sample rate\n",
    "    num_samples = tf.cast(tf.shape(wav)[0], dtype=tf.float32) * target_sample_rate / sample_rate\n",
    "    num_samples = tf.cast(num_samples, tf.int32)\n",
    "\n",
    "    # Resample the wav using scipy resample\n",
    "    resampled_wav = tf.numpy_function(resample_audio, [wav, num_samples], tf.float32)\n",
    "\n",
    "    # Pad or truncate to target length\n",
    "    resampled_wav = tf.cond(tf.shape(resampled_wav)[0] < target_length,\n",
    "                            lambda: tf.pad(resampled_wav, [[0, target_length - tf.shape(resampled_wav)[0]]]),\n",
    "                            lambda: resampled_wav[:target_length])\n",
    "\n",
    "    return resampled_wav\n",
    "\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(filename, label):\n",
    "    wav_data = load_wav_16k_mono(filename)\n",
    "    scores, embeddings, _ = yamnet_model(wav_data)\n",
    "    embeddings = tf.reduce_mean(embeddings, axis=0)  # Average across frames\n",
    "    return embeddings, label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get file paths and labels\n",
    "filenames, labels = get_file_paths_and_labels(main_directory, class_names)\n",
    "\n",
    "filenames_ds = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "labels_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "\n",
    "# Zipping the datasets to create pairs of (filename, label)\n",
    "main_ds = tf.data.Dataset.zip((filenames_ds, labels_ds))\n",
    "\n",
    "# Apply loading and preprocessing\n",
    "main_ds = main_ds.map(load_and_preprocess_data)\n",
    "\n",
    "# Splitting the dataset\n",
    "train_ds = main_ds.take(int(len(filenames) * 0.7))\n",
    "test_ds = main_ds.skip(int(len(filenames) * 0.7))\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 16)                4112      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 660240 (2.52 MB)\n",
      "Trainable params: 660240 (2.52 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Model definition\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "my_model = Sequential([\n",
    "    Input(shape=(1024,), dtype=tf.float32, name='input_embedding'),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5), # Adding dropout\n",
    "    Dense(256, activation='relu'), # Additional hidden layer\n",
    "    Dense(len(class_names))\n",
    "], name='my_model')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "my_model.summary()\n",
    "\n",
    "my_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.5284 - accuracy: 0.8083\n",
      "Epoch 2/50\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.4602 - accuracy: 0.8400\n",
      "Epoch 3/50\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.4954 - accuracy: 0.8309\n",
      "Epoch 4/50\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.4605 - accuracy: 0.8327\n",
      "Epoch 5/50\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.4315 - accuracy: 0.8517\n",
      "Epoch 6/50\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.4010 - accuracy: 0.8562\n",
      "Epoch 7/50\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.4031 - accuracy: 0.8535\n",
      "Epoch 8/50\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.3860 - accuracy: 0.8644\n",
      "Epoch 9/50\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.3770 - accuracy: 0.8580\n",
      "Epoch 10/50\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.3754 - accuracy: 0.8662\n",
      "Epoch 11/50\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.3673 - accuracy: 0.8707\n",
      "Epoch 12/50\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.3536 - accuracy: 0.8807\n",
      "Epoch 13/50\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.3239 - accuracy: 0.8752\n",
      "Epoch 14/50\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.3286 - accuracy: 0.8861\n",
      "Epoch 15/50\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.3212 - accuracy: 0.8843\n",
      "Epoch 16/50\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.3142 - accuracy: 0.8906\n",
      "Epoch 17/50\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.2999 - accuracy: 0.9014\n",
      "Epoch 18/50\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.2922 - accuracy: 0.9014\n",
      "Epoch 19/50\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.2842 - accuracy: 0.8996\n",
      "Epoch 20/50\n",
      "35/35 [==============================] - 0s 8ms/step - loss: 0.2963 - accuracy: 0.8987\n",
      "Epoch 21/50\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.3419 - accuracy: 0.8843\n",
      "Epoch 22/50\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.2640 - accuracy: 0.9024\n",
      "Epoch 23/50\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 0.2663 - accuracy: 0.9051\n",
      "Epoch 24/50\n",
      "35/35 [==============================] - 1s 18ms/step - loss: 0.2606 - accuracy: 0.9069\n",
      "Epoch 25/50\n",
      "35/35 [==============================] - 1s 13ms/step - loss: 0.2711 - accuracy: 0.9114\n",
      "Epoch 26/50\n",
      "35/35 [==============================] - 0s 12ms/step - loss: 0.2444 - accuracy: 0.9096\n",
      "Epoch 27/50\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.2304 - accuracy: 0.9231\n",
      "Epoch 28/50\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.2305 - accuracy: 0.9114\n",
      "Epoch 29/50\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.2505 - accuracy: 0.9042\n",
      "Epoch 30/50\n",
      "35/35 [==============================] - 0s 9ms/step - loss: 0.2552 - accuracy: 0.9132\n"
     ]
    }
   ],
   "source": [
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = my_model.fit(train_ds, epochs=50, callbacks=callback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 8ms/step - loss: 18.8742 - accuracy: 0.0274\n",
      "Loss:  18.87421989440918\n",
      "Accuracy:  0.027426160871982574\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = my_model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-02 12:46:43.486298: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: testing_wav_file_name; No such file or directory\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "{{function_node __wrapped__ReadFile_device_/job:localhost/replica:0/task:0/device:CPU:0}} testing_wav_file_name; No such file or directory [Op:ReadFile]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Test on a specific file (replace 'testing_wav_file_name' with a path to a WAV file)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m testing_wav_data \u001b[39m=\u001b[39m load_wav_16k_mono(\u001b[39m'\u001b[39;49m\u001b[39mtesting_wav_file_name\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m scores, embeddings, _ \u001b[39m=\u001b[39m yamnet_model(testing_wav_data)\n\u001b[1;32m      4\u001b[0m result \u001b[39m=\u001b[39m my_model(embeddings)\u001b[39m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn[69], line 20\u001b[0m, in \u001b[0;36mload_wav_16k_mono\u001b[0;34m(filename, target_length)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_wav_16k_mono\u001b[39m(filename, target_length\u001b[39m=\u001b[39m\u001b[39m16000\u001b[39m):\n\u001b[1;32m     19\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio, and pad to target length. \"\"\"\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     file_contents \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39;49mread_file(filename)\n\u001b[1;32m     21\u001b[0m     wav, sample_rate \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39maudio\u001b[39m.\u001b[39mdecode_wav(\n\u001b[1;32m     22\u001b[0m           file_contents,\n\u001b[1;32m     23\u001b[0m           desired_channels\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     24\u001b[0m     wav \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msqueeze(wav, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.8/site-packages/tensorflow/python/ops/io_ops.py:133\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(filename, name)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mio.read_file\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mio.read_file\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mread_file\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_file\u001b[39m(filename, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     98\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Reads the contents of file.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[1;32m    100\u001b[0m \u001b[39m  This operation returns a tensor with the entire contents of the input\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39m    A tensor of dtype \"string\", with the file contents.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_io_ops\u001b[39m.\u001b[39;49mread_file(filename, name)\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.8/site-packages/tensorflow/python/ops/gen_io_ops.py:582\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(filename, name)\u001b[0m\n\u001b[1;32m    580\u001b[0m   \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    581\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 582\u001b[0m   \u001b[39mreturn\u001b[39;00m read_file_eager_fallback(\n\u001b[1;32m    583\u001b[0m       filename, name\u001b[39m=\u001b[39;49mname, ctx\u001b[39m=\u001b[39;49m_ctx)\n\u001b[1;32m    584\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_SymbolicException:\n\u001b[1;32m    585\u001b[0m   \u001b[39mpass\u001b[39;00m  \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.8/site-packages/tensorflow/python/ops/gen_io_ops.py:605\u001b[0m, in \u001b[0;36mread_file_eager_fallback\u001b[0;34m(filename, name, ctx)\u001b[0m\n\u001b[1;32m    603\u001b[0m _inputs_flat \u001b[39m=\u001b[39m [filename]\n\u001b[1;32m    604\u001b[0m _attrs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m _result \u001b[39m=\u001b[39m _execute\u001b[39m.\u001b[39;49mexecute(\u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mReadFile\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m1\u001b[39;49m, inputs\u001b[39m=\u001b[39;49m_inputs_flat,\n\u001b[1;32m    606\u001b[0m                            attrs\u001b[39m=\u001b[39;49m_attrs, ctx\u001b[39m=\u001b[39;49mctx, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m _execute\u001b[39m.\u001b[39mmust_record_gradient():\n\u001b[1;32m    608\u001b[0m   _execute\u001b[39m.\u001b[39mrecord_gradient(\n\u001b[1;32m    609\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mReadFile\u001b[39m\u001b[39m\"\u001b[39m, _inputs_flat, _attrs, _result)\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mNotFoundError\u001b[0m: {{function_node __wrapped__ReadFile_device_/job:localhost/replica:0/task:0/device:CPU:0}} testing_wav_file_name; No such file or directory [Op:ReadFile]"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test on a specific file (replace 'testing_wav_file_name' with a path to a WAV file)\n",
    "testing_wav_data = load_wav_16k_mono('testing_wav_file_name')\n",
    "scores, embeddings, _ = yamnet_model(testing_wav_data)\n",
    "result = my_model(embeddings).numpy()\n",
    "\n",
    "inferred_class = class_names[result.mean(axis=0).argmax()]\n",
    "print(f'The main sound is: {inferred_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 2, 1024) (32,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-02 12:42:02.887027: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_ds.take(1):\n",
    "    print(x.shape, y.shape)  # x should be (32, 1024) and y should be (32,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
